{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Recurrent Q-Network\n",
    "\n",
    "이 노트북은 부분 관찰가능 마르코프 결정 프로세스를 풀 수 있는 Deep Recurrent Q-Network의 구현 예제를 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import tensorflow.contrib.slim as slim\n",
    "%matplotlib inline\n",
    "\n",
    "# helper 에는 타겟 신경망 그라디언트 업데이트 등이 들어있다.\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 게임 환경 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gridworld import gameEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "격자 세계의 크기는 조절 가능하다. 크기를 더 작게 만들면 DRQN 에이전트에게 쉬운 작업을 제공하고, 반대로 더 큰 세상은 도전과제가 된다.\n",
    "\n",
    "시야의 제한을 True로 설정하면, 부분 관찰가능 MDP가 된다. False 로 설정하면, 에이전트에게 전체 환경을 제공하는 완전한 MDP가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFiCAYAAAAna2l5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noP8PyT0wyat/clIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125fbc70a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MDP\n",
    "env = gameEnv(partial=False,size=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFiCAYAAAAna2l5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125fbce3d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# POMDP\n",
    "env = gameEnv(partial=True,size=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에는 우리 게임에서 시작 환경의 예이다. 에이전트는 파란 사각형을 조정하고, 위, 아래, 왼쪽, 오른쪽으로 이동가능하다. 목표는 초록 사각형(+1 보상)으로 이동하고 빨간 사각형(-1 보상)을 피하는 것이다. 에이전트가 초록이나 빨간 사각형으로 이동하면, 새로운 장소에 초기화된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size,rnn_cell,myScope):\n",
    "        # 신경망은 게임으로부터 벡터화된 배열로 프레임을 받아서,\n",
    "        # 이것을 리사이즈 하고, 4개의 콘볼루션을 통해 처리한다.\n",
    "        \n",
    "        # 입력값을 받는 부분 21168 차원은 84*84*3 의 차원이다.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        # conv2d 처리를 위해 84x84x3 으로 다시 리사이즈\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        \n",
    "        # 첫번째 콘볼루션은 8x8 커널을 4 스트라이드로 32개의 activation map을 만든다\n",
    "        # 출력 크기는 (image 크기 - 필터 크기) / 스트라이드 + 1 이다.\n",
    "        # zero padding이 없는 VALID 옵션이기 때문에\n",
    "        # (84-8)/4 + 1\n",
    "        # 20x20x32 의 activation volumn이 나온다\n",
    "        self.conv1 = slim.convolution2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,\\\n",
    "            kernel_size=[8,8],stride=[4,4],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv1')\n",
    "        \n",
    "        # 두번째 콘볼루션은 4x4 커널을 2 스트라이드로 64개의 activation map을 만든다.\n",
    "        # 출력 크기는 9x9x64\n",
    "        self.conv2 = slim.convolution2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,\\\n",
    "            kernel_size=[4,4],stride=[2,2],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv2')\n",
    "        \n",
    "        # 세번째 콘볼루션은 3x3 커널을 1 스트라이드로 64개의 activation map을 만든다.\n",
    "        # 출력 크기는 7x7x64\n",
    "        self.conv3 = slim.convolution2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,\\\n",
    "            kernel_size=[3,3],stride=[1,1],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv3')\n",
    "        \n",
    "        # 네번째 콘볼루션은 7x7 커널을 1 스트라이드 512개의 activation map을 만든다.\n",
    "        # 출력 크기는 1x1x512\n",
    "        self.conv4 = slim.convolution2d( \\\n",
    "            inputs=self.conv3,num_outputs=512,\\\n",
    "            kernel_size=[7,7],stride=[1,1],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv4')\n",
    "        \n",
    "        \n",
    "        # 몇개의 걸음 기록을 사용할지 받아들이는 부분\n",
    "        self.trainLength = tf.placeholder(dtype=tf.int32)\n",
    "        \n",
    "        # 마지막 콘볼루션 레이어로부터의 출력을 받아, 순환 레이어로 보낸다.\n",
    "        # 이 입력은 rnn 처리과정을 위해 [배치 x 걸음 기록(trace) x rnn hidden node unit 수] 로 크기 조정 되어야만 한다.\n",
    "        # 그리고 rnn의 윗부분을 통과해 [batch x units]을 반환한다\n",
    "        \n",
    "        # 배치 사이즈를 받는 부분\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32)\n",
    "        # 콘볼루션의 마지막 부분 1x1x512 를 512 로 바꾸고, 이를 batch x trace x hidden node 로 바꿈\n",
    "        self.convFlat = tf.reshape(slim.flatten(self.conv4),[self.batch_size,self.trainLength,h_size])\n",
    "        # rnn hidden node의 초기 상태를 0으로 초기화\n",
    "        self.state_in = cell.zero_state(self.batch_size, tf.float32)\n",
    "        # dynamic_rnn 을 이용해 rnn output과 다음 상태를 반환함\n",
    "        self.rnn,self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                inputs=self.convFlat,cell=rnn_cell,dtype=tf.float32,initial_state=self.state_in,scope=myScope+'_rnn')\n",
    "        # 벡터화해줌\n",
    "        self.rnn = tf.reshape(self.rnn,shape=[-1,h_size])\n",
    "        \n",
    "        # 순환 레이어의 결과를 절반으로 나눠서 가치와 이득 흐름으로 넣어줌\n",
    "        self.streamA,self.streamV = tf.split(1,2,self.rnn)\n",
    "        self.AW = tf.Variable(tf.random_normal([int(h_size/2),4]))\n",
    "        self.VW = tf.Variable(tf.random_normal([int(h_size/2),1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        # 가치 함수 값에 이득에서 이득의 평균을 빼준 값들을 더해준다.\n",
    "        self.Qout = self.Value + tf.sub(self.Advantage,tf.reduce_mean(self.Advantage,reduction_indices=1,keep_dims=True))\n",
    "        # 이것으로 행동을 고른다.\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        # 타겟과 예측 Q value 사이의 차이의 제곱합이 손실이다.\n",
    "        # 타겟Q를 받는 부분\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        # 행동을 받는 부분\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        # 행동을 one_hot 인코딩 하는 부분\n",
    "        self.actions_onehot = tf.one_hot(self.actions,4,dtype=tf.float32)\n",
    "        \n",
    "        # 각 네트워크의 행동의 Q 값을 골라내는 것\n",
    "        # action 번째를 뽑고 싶지만 tensor는 인덱스로 쓸 수 없어서 이렇게 하는듯(내 생각)\n",
    "        self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        # 각각의 차이\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        \n",
    "        #신경망을 통해 정확한 그라디언트만 보내기 위해, Lample & Chatlot 2016 에서 각 기록에 대한 손실의 첫 절반을 마스크 할 것이다.\n",
    "        self.maskA = tf.zeros([self.batch_size,4]) # 4는 trace 의 수\n",
    "        self.maskB = tf.ones([self.batch_size,4])\n",
    "        self.mask = tf.concat(1,[self.maskA,self.maskB])\n",
    "        self.mask = tf.reshape(self.mask,[-1])\n",
    "        \n",
    "        # 뒤에 절반만 가지고 손실을 계산한다\n",
    "        self.loss = tf.reduce_mean(self.td_error * self.mask)\n",
    "        \n",
    "        # 최적화 방법 adam\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        # 업데이트 함수\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경험 재생 (Experience Replay)\n",
    "\n",
    "이들 클래스는 경험들을 저장하고 신경망을 학습하기 위해 무작위로 샘플링 한다. Episode buffer는 각 개별 에피소드에 대한 경험을 저장한다. Experience buffer 는 경험의 전체 에피소드를 저장하고, sample() 함수를 통해 신경망에서 필요한 학습 배치를 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 1000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    # 더할 때 버퍼사이즈를 넘으면, 앞에서부터 지우고 다시 넣는다.\n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self,batch_size,trace_length):\n",
    "        sampled_episodes = random.sample(self.buffer,batch_size)\n",
    "        sampledTraces = []\n",
    "        # 이전과 다른 부분, 샘플로 뽑힌 에피소드에서 지정된 크기만큼의 걸음(프레임)을 붙여서 가져온다.\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0,len(episode)+1-trace_length)\n",
    "            sampledTraces.append(episode[point:point+trace_length])\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        return np.reshape(sampledTraces,[batch_size*trace_length,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 학습 파라미터 설정하기\n",
    "batch_size = 4 # 각 학습 단계에서 경험 기록을 몇개나 사용할지\n",
    "trace_length = 8 # 학습할 때 각 경험 기록을 얼마나 길게 사용할지\n",
    "update_freq = 5 # 학습 단계를 얼마나 자주 수행할지\n",
    "y = .99 # 타겟 Q value 에 대한 할인 인자\n",
    "startE = 1 # 무작위 행위의 시작 확률\n",
    "endE = 0.1 # 무작위 행위의 최종 확률\n",
    "anneling_steps = 10000 # startE부터 endE까지 몇단계에 걸쳐서 줄일 것인가.\n",
    "num_episodes = 10000 # 몇개의 에피소드를 할 것인가.\n",
    "pre_train_steps = 10000 # 학습 시작 전에 몇번의 무작위 행위를 할 것인가.\n",
    "load_model = False # 저장된 모델을 불러올 것인가?\n",
    "path = \"./drqn\" # 모델을 저장할 위치\n",
    "h_size = 512 # 이득 함수와 가치 함수로 나뉘기 전에 최종 콘볼루션의 크기\n",
    "max_epLength = 50 # 에피소드의 최대 길이 (50 걸음)\n",
    "time_per_step = 1 # git 생성에 사용될 각 걸음의 크기 \n",
    "summaryLength = 100 # 분석을 위해 주기적으로 저장하기 위한 에피소드의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Set Success\n",
      "5000 0.57 1\n",
      "10000 0.33 1\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "15000 1.2 0.5499999999998275\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "20000 2.14 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "25000 2.02 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "30000 2.92 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "35000 3.37 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "40000 3.21 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "45000 3.54 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "50000 3.72 0.09999999999985551\n",
      "Saved Model\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "55000 3.81 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "60000 3.12 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "65000 3.93 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "70000 2.33 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "75000 3.63 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "80000 4.05 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "85000 3.33 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "90000 5.02 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "95000 5.13 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "100000 4.91 0.09999999999985551\n",
      "Saved Model\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "105000 2.78 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "110000 3.94 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "115000 5.18 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "120000 4.58 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "125000 4.59 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "130000 4.87 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "135000 4.35 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "140000 4.59 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "145000 5.2 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "150000 5.15 0.09999999999985551\n",
      "Saved Model\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "155000 4.58 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "160000 4.32 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "165000 4.32 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "170000 4.46 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "175000 5.06 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "180000 4.41 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "185000 4.77 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "190000 4.57 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "195000 5.14 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "200000 4.88 0.09999999999985551\n",
      "Saved Model\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "205000 4.71 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "210000 5.01 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "215000 5.64 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "220000 6.11 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "225000 5.38 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "230000 4.32 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "235000 5.54 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "240000 4.82 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "245000 5.18 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "250000 4.99 0.09999999999985551\n",
      "Saved Model\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "255000 5.4 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "260000 5.1 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "265000 4.88 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "270000 4.93 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "275000 5.2 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "280000 5.17 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "285000 5.35 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "290000 5.63 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "295000 5.37 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "300000 5.57 0.09999999999985551\n",
      "Saved Model\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "305000 5.3 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "310000 5.43 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "315000 4.77 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "320000 4.82 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "325000 5.13 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "330000 4.92 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "335000 5.46 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "340000 5.5 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "345000 5.03 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "350000 5.37 0.09999999999985551\n",
      "Saved Model\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "355000 5.33 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "360000 5.49 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "365000 5.17 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "370000 5.66 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "375000 5.67 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "380000 5.45 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "385000 5.14 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "390000 4.82 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "395000 5.0 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "400000 5.43 0.09999999999985551\n",
      "Saved Model\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "405000 5.31 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "410000 5.17 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "415000 4.95 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "420000 5.37 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "425000 5.27 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "430000 5.5 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "435000 4.82 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "440000 5.41 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "445000 5.12 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "450000 4.58 0.09999999999985551\n",
      "Saved Model\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "455000 5.07 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "460000 5.29 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "465000 5.5 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "470000 6.01 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "475000 5.55 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "480000 5.12 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "485000 6.11 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "490000 5.46 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "495000 6.35 0.09999999999985551\n",
      "Target network updated.\n",
      "Target Set Success\n",
      "500000 5.66 0.09999999999985551\n"
     ]
    }
   ],
   "source": [
    "# 그래프를 초기화한다\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 주요 신경망과 타겟 q-network를 위한 rnn 셀들을 정의한다.\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellT = tf.nn.rnn_cell.LSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size,cell,'main')\n",
    "targetQN = Qnetwork(h_size,cellT,'target')\n",
    "\n",
    "# 변수들을 초기화한다\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# gpu 에러를 방지\n",
    "config = tf.ConfigProto(\n",
    "        device_count={'GPU': 0}  # uncomment this line to force CPU\n",
    "    )\n",
    "\n",
    "# saver를 만든다\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "# 학습가능한 변수를 꺼낸다\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "# 타겟 신경망을 업데이트하기 위한 값을 만든다\n",
    "targetOps = updateTargetGraph(trainables)\n",
    "\n",
    "# 경험을 저장할 장소\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# 무작위 행위 확률을 설정한다\n",
    "e = startE\n",
    "# 점점 줄여나간다.\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "# 에피소드별 총 보상과 걸음을 저장할 리스트를 만든다\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "# 모델을 세이브할 장소를 만든다.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# 컨트롤 센터를 위한 로그파일을 만드는 것인데, 나는 컨트롤 센터가 없으니 안 만든다.\n",
    "##Write the first line of the master log-file for the Control Center\n",
    "# with open('./Center/log.csv', 'w') as myfile:\n",
    "#     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#     wr.writerow(['Episode','Length','Reward','IMG','LOG','SAL'])    \n",
    "  \n",
    "# 텐서플로 세션을 연다\n",
    "with tf.Session(config=config) as sess:\n",
    "    # 모델을 불러올지 체크\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        # 모델을 불러온다\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    # 변수를 초기화한다.\n",
    "    sess.run(init)\n",
    "    \n",
    "    # 주요 신경망과 동일하게 타겟 신경망을 설정한다\n",
    "    updateTarget(targetOps,sess)\n",
    "    \n",
    "    # 텐서보드를 위해 요약 변수들을 저장\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('./train',\n",
    "                                    sess.graph)\n",
    "    \n",
    "    # 에피소드 시작\n",
    "    for i in range(num_episodes):\n",
    "        # 에피소드별 경험 버퍼를 초기화한다\n",
    "        episodeBuffer = []\n",
    "        # 환경과 처음 상태을 초기화한다\n",
    "        sP = env.reset()\n",
    "        s = processState(sP)\n",
    "        # 종료 여부\n",
    "        d = False\n",
    "        # 보상\n",
    "        rAll = 0\n",
    "        # 걸음\n",
    "        j = 0\n",
    "        # 순환 레이어의 은닉 상태를 초기화한다.\n",
    "        state = (np.zeros([1,h_size]),np.zeros([1,h_size]))\n",
    "        # Q-Network\n",
    "        # 만약 50 걸음보다 더 간다면 종료한다.\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: \n",
    "            j+=1\n",
    "            # Q-network 로부터 행동을 greedy 하게 선택하거나 e의 확률로 무작위 행동을 한다\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                state1 = sess.run(mainQN.rnn_state,\\\n",
    "                    feed_dict={mainQN.scalarInput:[s/255.0],mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                # 신경망을 통해 Q 값을 가져오는 부분\n",
    "                a, state1 = sess.run([mainQN.predict,mainQN.rnn_state],\\\n",
    "                    feed_dict={mainQN.scalarInput:[s/255.0],mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = a[0]\n",
    "            # 주어진 행동을 실행하고 다음 상태, 보상, 종료 여부를 가져옴\n",
    "            s1P,r,d = env.step(a)\n",
    "            # 상태를 다시 21168 차원으로 리사이즈\n",
    "            s1 = processState(s1P)\n",
    "            # 걸음수를 늘림\n",
    "            total_steps += 1\n",
    "            # 버퍼에 현재 상태, 행동, 보상, 다음 상태, 종료 여부를 저장한다\n",
    "            episodeBuffer.append(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n",
    "            \n",
    "            # 무작위 행동의 수를 넘으면 시작\n",
    "            if total_steps > pre_train_steps:\n",
    "                # 무작위 확률 값을 줄인다\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                # 총 걸음이 업데이트 수로 나누어 떨어지면 시작\n",
    "                if total_steps % (update_freq*1000) == 0:\n",
    "                    print (\"Target network updated.\")\n",
    "                    updateTarget(targetOps,sess)\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    # 순환 레이어의 은닉층을 초기화한다.\n",
    "                    state_train = (np.zeros([batch_size,h_size]),np.zeros([batch_size,h_size])) \n",
    "                    \n",
    "                    # 경험으로부터 랜덤한 배치를 뽑는다\n",
    "                    trainBatch = myBuffer.sample(batch_size,trace_length)\n",
    "                    # 아래는 target Q-value를 업데이트하는 Double-DQN을 수행한다\n",
    "                    # 주요 신경망에서 행동을 고른다.\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={\\\n",
    "                        mainQN.scalarInput:np.vstack(trainBatch[:,3]/255.0),\\\n",
    "                        mainQN.trainLength:trace_length,mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "                    # 타겟 신경망에서 Q 값들을 얻는다.\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={\\\n",
    "                        targetQN.scalarInput:np.vstack(trainBatch[:,3]/255.0),\\\n",
    "                        targetQN.trainLength:trace_length,targetQN.state_in:state_train,targetQN.batch_size:batch_size})\n",
    "                    # 종료 여부에 따라 가짜 라벨을 만들어준다\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    # 타겟 신경망의 Q 값들 중에 주요 신경망에서 고른 행동 번째의 Q 값들을 가져온다.(이부분이 doubleQ)\n",
    "                    doubleQ = Q2[range(batch_size*trace_length),Q1]\n",
    "                    # 보상에 대한 더블 Q 값을 더해준다. y는 할인 인자\n",
    "                    # targetQ 는 즉각적인 보상 + 다음 상태의 최대 보상(doubleQ)\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    # 우리의 타겟 값들과 함께 신경망을 업데이트해준다.\n",
    "                    # 행동들에 대해서 targetQ 값과의 차이를 통해 손실을 구하고 업데이트\n",
    "                    sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]/255.0),mainQN.targetQ:targetQ,\\\n",
    "                        mainQN.actions:trainBatch[:,1],mainQN.trainLength:trace_length,\\\n",
    "                        mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "            # 총 보상\n",
    "            rAll += r\n",
    "            # 상태를 바꾼다\n",
    "            s = s1\n",
    "            sP = s1P\n",
    "            # 은닉 상태를 바꾼다\n",
    "            state = state1\n",
    "            # 종료\n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "\n",
    "        # 경험 버퍼에 에피소드를 추가한다\n",
    "        bufferArray = np.array(episodeBuffer)\n",
    "        episodeBuffer = bufferArray\n",
    "        myBuffer.add(episodeBuffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "\n",
    "        # 주기적으로 모델을 저장한다\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model\")\n",
    "        if len(rList) % summaryLength == 0 and len(rList) != 0:\n",
    "            print (total_steps,np.mean(rList[-summaryLength:]), e)\n",
    "#             saveToCenter(i,rList,jList,np.reshape(np.array(episodeBuffer),[len(episodeBuffer),5]),\\\n",
    "#                 summaryLength,h_size,sess,mainQN,time_per_step)\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = 0.01\n",
    "num_episodes = 10000\n",
    "load_model = True\n",
    "path = \"./drqn\"\n",
    "h_size = 512\n",
    "max_epLength = 50 \n",
    "time_per_step = 1\n",
    "summaryLength = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "5000 6.53 0.01\n",
      "10000 6.41 0.01\n",
      "15000 6.67 0.01\n",
      "20000 7.39 0.01\n",
      "25000 6.54 0.01\n",
      "30000 6.83 0.01\n",
      "35000 6.63 0.01\n",
      "40000 6.58 0.01\n",
      "45000 6.98 0.01\n",
      "50000 6.87 0.01\n",
      "55000 6.41 0.01\n",
      "60000 6.5 0.01\n",
      "65000 6.94 0.01\n",
      "70000 6.54 0.01\n",
      "75000 6.91 0.01\n",
      "80000 6.9 0.01\n",
      "85000 6.38 0.01\n",
      "90000 6.57 0.01\n",
      "95000 6.69 0.01\n",
      "100000 6.37 0.01\n",
      "105000 6.61 0.01\n",
      "110000 6.7 0.01\n",
      "115000 6.32 0.01\n",
      "120000 6.69 0.01\n",
      "125000 6.92 0.01\n",
      "130000 6.55 0.01\n",
      "135000 6.26 0.01\n",
      "140000 6.57 0.01\n",
      "145000 6.69 0.01\n",
      "150000 6.53 0.01\n",
      "155000 6.47 0.01\n",
      "160000 6.76 0.01\n",
      "165000 6.84 0.01\n",
      "170000 6.73 0.01\n",
      "175000 6.76 0.01\n",
      "180000 6.54 0.01\n",
      "185000 6.6 0.01\n",
      "190000 6.89 0.01\n",
      "195000 6.75 0.01\n",
      "200000 6.82 0.01\n",
      "205000 6.78 0.01\n",
      "210000 6.47 0.01\n",
      "215000 6.46 0.01\n",
      "220000 6.72 0.01\n",
      "225000 6.25 0.01\n",
      "230000 6.75 0.01\n",
      "235000 6.56 0.01\n",
      "240000 6.85 0.01\n",
      "245000 6.65 0.01\n",
      "250000 6.9 0.01\n",
      "255000 6.73 0.01\n",
      "260000 6.36 0.01\n",
      "265000 6.36 0.01\n",
      "270000 6.43 0.01\n",
      "275000 6.8 0.01\n",
      "280000 6.19 0.01\n",
      "285000 6.61 0.01\n",
      "290000 6.68 0.01\n",
      "295000 6.54 0.01\n",
      "300000 6.6 0.01\n",
      "305000 6.77 0.01\n",
      "310000 6.66 0.01\n",
      "315000 7.37 0.01\n",
      "320000 6.81 0.01\n",
      "325000 6.85 0.01\n",
      "330000 6.9 0.01\n",
      "335000 6.91 0.01\n",
      "340000 6.38 0.01\n",
      "345000 6.52 0.01\n",
      "350000 7.03 0.01\n",
      "355000 6.64 0.01\n",
      "360000 6.79 0.01\n",
      "365000 7.03 0.01\n",
      "370000 7.09 0.01\n",
      "375000 6.75 0.01\n",
      "380000 6.46 0.01\n",
      "385000 6.7 0.01\n",
      "390000 6.72 0.01\n",
      "395000 6.65 0.01\n",
      "400000 6.39 0.01\n",
      "405000 6.78 0.01\n",
      "410000 7.34 0.01\n",
      "415000 6.87 0.01\n",
      "420000 6.76 0.01\n",
      "425000 6.75 0.01\n",
      "430000 6.05 0.01\n",
      "435000 7.13 0.01\n",
      "440000 6.57 0.01\n",
      "445000 6.44 0.01\n",
      "450000 6.69 0.01\n",
      "455000 6.35 0.01\n",
      "460000 6.76 0.01\n",
      "465000 6.88 0.01\n",
      "470000 6.59 0.01\n",
      "475000 6.63 0.01\n",
      "480000 6.62 0.01\n",
      "485000 6.6 0.01\n",
      "490000 6.61 0.01\n",
      "495000 7.13 0.01\n",
      "500000 6.39 0.01\n",
      "Percent of succesful episodes: 6.6769%\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellT = tf.nn.rnn_cell.LSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size,cell,'main')\n",
    "targetQN = Qnetwork(h_size,cellT,'target')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "        device_count={'GPU': 0}  # uncomment this line to force CPU\n",
    "    )\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "##Write the first line of the master log-file for the Control Center\n",
    "# with open('./Center/log.csv', 'w') as myfile:\n",
    "#     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#     wr.writerow(['Episode','Length','Reward','IMG','LOG','SAL'])    \n",
    "    \n",
    "    #wr = csv.writer(open('./Center/log.csv', 'a'), quoting=csv.QUOTE_ALL)\n",
    "with tf.Session(config=config) as sess:\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "\n",
    "        \n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = []\n",
    "        sP = env.reset()\n",
    "        s = processState(sP)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        state = (np.zeros([1,h_size]),np.zeros([1,h_size]))\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: \n",
    "            j+=1\n",
    "            \n",
    "            if np.random.rand(1) < e:\n",
    "                state1 = sess.run(mainQN.rnn_state,\\\n",
    "                    feed_dict={mainQN.scalarInput:[s/255.0],mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a, state1 = sess.run([mainQN.predict,mainQN.rnn_state],\\\n",
    "                    feed_dict={mainQN.scalarInput:[s/255.0],mainQN.trainLength:1,\\\n",
    "                    mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = a[0]\n",
    "            s1P,r,d = env.step(a)\n",
    "            s1 = processState(s1P)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.append(np.reshape(np.array([s,a,r,s1,d]),[1,5])) #Save the experience to our episode buffer.\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            sP = s1P\n",
    "            state = state1\n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "\n",
    "        bufferArray = np.array(episodeBuffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "\n",
    "        if len(rList) % summaryLength == 0 and len(rList) != 0:\n",
    "            print (total_steps,np.mean(rList[-summaryLength:]), e)\n",
    "#             saveToCenter(i,rList,jList,np.reshape(np.array(episodeBuffer),[len(episodeBuffer),5]),\\\n",
    "#                 summaryLength,h_size,sess,mainQN,time_per_step)\n",
    "print (\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
