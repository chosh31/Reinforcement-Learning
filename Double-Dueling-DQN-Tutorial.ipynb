{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond\n",
    "\n",
    "이 iPython noteboon 에서 Double DQN 과 Dueling DQN 둘다 사용해서 Deep Q-Network를 구현한다. 이 에이전트는 기본적인 격자 세계에서 네비게이션 작업을 해결하는 것을 배운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 게임 환경 불러오기\n",
    "\n",
    "격자세계의 크기를 조절할 수 있다. 크기를 더 작게 하면 우리의 DQN 에이전트가 쉽게 작업할 수 있지만, 크게하면 도전과제가 된다.\n",
    "\n",
    "gridworld 모듈은 https://github.com/awjuliani/DeepRL-Agents/blob/master/gridworld.py 에서 받을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ea1efabb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "env = gameEnv(partial=False,size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에는 우리의 단순한 게임에서 출발 환경의 예이다. 에이전트는 파란색 사각형을 조정하고, 위, 아래, 왼쪽, 오른쪽으로 움직일 수 있다. 목표는 초록색 사각형(보상 +1)로 움직이고, 빨간색 사각형(보상 -1)을 피하는 것이다. 이 세 블록의 위치는 매 에피소드 마다 무작위이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 신경망 구현\n",
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        # 신경망은 게임으로부터 벡터화된 배열로 프레임을 받아서 \n",
    "        # 이것을 리사이즈 하고, 4개의 콘볼루션 레이어를 통해 처리한다.\n",
    "        \n",
    "        # 입력값을 받는 부분 21168 차원은 84*84*3 의 차원이다.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        # conv2d 처리를 위해 84x84x3 으로 다시 리사이즈\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        \n",
    "        # 첫번째 콘볼루션은 8x8 커널을 4 스트라이드로 32개의 activation map을 만든다\n",
    "        # 출력 크기는 (image 크기 - 필터 크기) / 스트라이드 + 1 이다.\n",
    "        # zero padding이 없는 VALID 옵션이기 때문에\n",
    "        # (84-8)/4 + 1\n",
    "        # 20x20x32 의 activation volumn이 나온다\n",
    "        self.conv1 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        # 두번째 콘볼루션은 4x4 커널을 2 스트라이드로 64개의 activation map을 만든다.\n",
    "        # 출력 크기는 9x9x64\n",
    "        self.conv2 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        # 세번째 콘볼루션은 3x3 커널을 1 스트라이드로 64개의 activation map을 만든다.\n",
    "        # 출력 크기는 7x7x64\n",
    "        self.conv3 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        # 네번째 콘볼루션은 7x7 커널을 1 스트라이드 512개의 activation map을 만든다.\n",
    "        # 출력 크기는 1x1x512\n",
    "        self.conv4 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv3,num_outputs=512,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        # 마지막 콘볼루션 레이어의 출력을 가지고 2로 나눈다.\n",
    "        # streamAC, streamVC 는 각각 1x1x256\n",
    "        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)\n",
    "        # 이를 벡터화한다. streamA 와 streamV는 256 차원씩이다.\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        # 256개의 노드를 곱해서 각각 A와 V를 구하는 가중치\n",
    "        self.AW = tf.Variable(tf.random_normal([256,env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([256,1]))\n",
    "        # 점수화 한다.\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        # 가치 함수 값에 이득에서 이득의 평균을 빼준 값들을 더해준다.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,reduction_indices=1,keep_dims=True))\n",
    "        # 이것으로 행동을 고른다.\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        # 타겟과 예측 Q value 사이의 차이의 제곱합이 손실이다.\n",
    "        # 타겟Q를 받는 부분\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        # 행동을 받는 부분\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        # 행동을 one_hot 인코딩 하는 부분 (tf.one_hot은 내 컴퓨터에서 GPU 에러를 내기에 다음의 해법을 찾아 적용)\n",
    "        def one_hot_patch(x,depth):\n",
    "            sparse_labels=tf.reshape(x,[-1,1])\n",
    "            derived_size=tf.shape(sparse_labels)[0]\n",
    "            indices=tf.reshape(tf.range(0,derived_size,1),[-1,1])\n",
    "            concated=tf.concat([indices,sparse_labels],1)\n",
    "            outshape=tf.concat([tf.reshape(derived_size,[1]),tf.reshape(depth,[1])],0)\n",
    "            return tf.sparse_to_dense(concated, outshape,1.0,0.0)\n",
    "        self.actions_onehot = one_hot_patch(self.actions,env.actions)\n",
    "        \n",
    "        # 각 네트워크의 행동의 Q 값을 골라내는 것\n",
    "        # action 번째를 뽑고 싶지만 tensor는 인덱스로 쓸 수 없어서 이렇게 하는듯(내 생각)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        # 각각의 차이\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        # 손실\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        # 최적화 방법 adam\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        # 업데이트 함수\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay\n",
    "\n",
    "이 클래스는 경험을 저장하고 샘플을 뽑아 신경망에 랜덤하게 보내진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    # 더할 때 버퍼사이즈를 넘으면, 앞에서부터 지우고 다시 넣는다.\n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "    # \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 게임 프레임들을 리사이즈 해주는 단순한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 함수들은 주요 신경망과 함께 타겟 신경망의 파라미터들을 업데이트 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    # tfVars 는 학습 가능한 변수들\n",
    "    # tau는 타겟 신경망이 학습 신경망을 향하는 비율\n",
    "    # 학습 가능한 변수들의 수\n",
    "    total_vars = len(tfVars)\n",
    "    \n",
    "    # 연산자 저장 리스트\n",
    "    op_holder = []\n",
    "    # 학습 가능한 변수의 절반은 주요 신경망, 절반은 타겟 신경망\n",
    "    for idx,var in enumerate(tfVars[0:int(total_vars/2)]):\n",
    "        # 앞의 절반의 값에 tau 값을 곱하면, 주요 신경망의 weight에 곱해지고\n",
    "        # 뒤의 절반의 값에 1-tau 값을 곱하면, 타겟 신경망의 weight에 곱해져서\n",
    "        # 이부분 타겟 신경망을 업데이트하는 부분\n",
    "        op_holder.append(tfVars[int(idx)+int(total_vars/2)].assign((var.value()*tau) \n",
    "                                                                   + ((1-tau)*tfVars[int(idx)+int(total_vars/2)].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "학습 파라미터들 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # 각 학습 단계에 대해 얼마나 많은 경험을 사용할지 결정\n",
    "update_freq = 4 # 학습 단계를 얼마나 자주 수행할 것인가\n",
    "y = .99 # 타겟 Q 값에 대한 할인 인자\n",
    "startE = 1 # 무작위 행위의 시작 확률\n",
    "endE = 0.1 # 무작위 행위의 최종 확률\n",
    "anneling_steps = 10000. # startE부터 endE까지 몇단계에 걸쳐서 줄일 것인가.\n",
    "num_episodes = 10000 # 몇개의 에피소드를 할 것인가.\n",
    "pre_train_steps = 10000 # 학습 시작 전에 몇번의 무작위 행위를 할 것인가.\n",
    "max_epLength = 50 # 에피소드의 최대 길이 (50 걸음)\n",
    "load_model = False # 저장된 모델을 불러올 것인가?\n",
    "path = \"./dqn\" # 모델을 저장할 위치\n",
    "h_size = 512 # 이득 함수와 가치 함수로 나뉘기 전에 최종 콘볼루션의 크기\n",
    "tau = 0.001 # 주요 신경망을 향해 타겟 신경망이 업데이트되는 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "500 2.0 1\n",
      "1000 3.3 1\n",
      "1500 1.0 1\n",
      "2000 0.3 1\n",
      "2500 2.9 1\n",
      "3000 2.0 1\n",
      "3500 0.5 1\n",
      "4000 2.3 1\n",
      "4500 3.0 1\n",
      "5000 1.2 1\n",
      "5500 3.1 1\n",
      "6000 1.9 1\n",
      "6500 1.5 1\n",
      "7000 1.1 1\n",
      "7500 1.4 1\n",
      "8000 1.9 1\n",
      "8500 1.5 1\n",
      "9000 1.9 1\n",
      "9500 4.0 1\n",
      "10000 2.6 1\n",
      "10500 2.2 0.9549999999999828\n",
      "11000 1.3 0.9099999999999655\n",
      "11500 2.1 0.8649999999999483\n",
      "12000 1.6 0.819999999999931\n",
      "12500 0.0 0.7749999999999138\n",
      "13000 2.2 0.7299999999998965\n",
      "13500 2.9 0.6849999999998793\n",
      "14000 0.9 0.639999999999862\n",
      "14500 2.2 0.5949999999998448\n",
      "15000 0.8 0.5499999999998275\n",
      "15500 0.8 0.5049999999998103\n",
      "16000 1.3 0.4599999999998177\n",
      "16500 1.3 0.41499999999982823\n",
      "17000 2.4 0.36999999999983874\n",
      "17500 1.3 0.32499999999984924\n",
      "18000 0.2 0.27999999999985975\n",
      "18500 1.9 0.23499999999986562\n",
      "19000 0.3 0.18999999999986225\n",
      "19500 0.7 0.14499999999985888\n",
      "20000 1.4 0.09999999999985551\n",
      "20500 0.6 0.09999999999985551\n",
      "21000 0.9 0.09999999999985551\n",
      "21500 1.0 0.09999999999985551\n",
      "22000 0.1 0.09999999999985551\n",
      "22500 1.8 0.09999999999985551\n",
      "23000 0.8 0.09999999999985551\n",
      "23500 1.2 0.09999999999985551\n",
      "24000 0.5 0.09999999999985551\n",
      "24500 0.7 0.09999999999985551\n",
      "25000 1.6 0.09999999999985551\n",
      "25500 1.3 0.09999999999985551\n",
      "26000 1.0 0.09999999999985551\n",
      "26500 0.6 0.09999999999985551\n",
      "27000 0.9 0.09999999999985551\n",
      "27500 1.1 0.09999999999985551\n",
      "28000 1.1 0.09999999999985551\n",
      "28500 0.6 0.09999999999985551\n",
      "29000 1.0 0.09999999999985551\n",
      "29500 1.8 0.09999999999985551\n",
      "30000 0.7 0.09999999999985551\n",
      "30500 0.6 0.09999999999985551\n",
      "31000 0.8 0.09999999999985551\n",
      "31500 1.9 0.09999999999985551\n",
      "32000 1.3 0.09999999999985551\n",
      "32500 1.8 0.09999999999985551\n",
      "33000 1.3 0.09999999999985551\n",
      "33500 0.7 0.09999999999985551\n",
      "34000 1.9 0.09999999999985551\n",
      "34500 1.8 0.09999999999985551\n",
      "35000 1.1 0.09999999999985551\n",
      "35500 1.2 0.09999999999985551\n",
      "36000 1.5 0.09999999999985551\n",
      "36500 1.6 0.09999999999985551\n",
      "37000 0.6 0.09999999999985551\n",
      "37500 1.3 0.09999999999985551\n",
      "38000 1.1 0.09999999999985551\n",
      "38500 1.3 0.09999999999985551\n",
      "39000 0.7 0.09999999999985551\n",
      "39500 1.2 0.09999999999985551\n",
      "40000 0.5 0.09999999999985551\n",
      "40500 1.3 0.09999999999985551\n",
      "41000 1.3 0.09999999999985551\n",
      "41500 1.8 0.09999999999985551\n",
      "42000 1.3 0.09999999999985551\n",
      "42500 0.7 0.09999999999985551\n",
      "43000 2.2 0.09999999999985551\n",
      "43500 1.7 0.09999999999985551\n",
      "44000 3.0 0.09999999999985551\n",
      "44500 1.6 0.09999999999985551\n",
      "45000 2.0 0.09999999999985551\n",
      "45500 1.0 0.09999999999985551\n",
      "46000 2.0 0.09999999999985551\n",
      "46500 2.2 0.09999999999985551\n",
      "47000 1.7 0.09999999999985551\n",
      "47500 1.0 0.09999999999985551\n",
      "48000 2.0 0.09999999999985551\n",
      "48500 2.7 0.09999999999985551\n",
      "49000 2.0 0.09999999999985551\n",
      "49500 2.1 0.09999999999985551\n",
      "50000 3.6 0.09999999999985551\n",
      "Saved Model\n",
      "50500 1.8 0.09999999999985551\n",
      "51000 1.0 0.09999999999985551\n",
      "51500 2.4 0.09999999999985551\n",
      "52000 1.8 0.09999999999985551\n",
      "52500 1.5 0.09999999999985551\n",
      "53000 2.5 0.09999999999985551\n",
      "53500 1.5 0.09999999999985551\n",
      "54000 0.6 0.09999999999985551\n",
      "54500 1.7 0.09999999999985551\n",
      "55000 1.9 0.09999999999985551\n",
      "55500 1.2 0.09999999999985551\n",
      "56000 1.7 0.09999999999985551\n",
      "56500 1.9 0.09999999999985551\n",
      "57000 2.7 0.09999999999985551\n",
      "57500 2.0 0.09999999999985551\n",
      "58000 1.7 0.09999999999985551\n",
      "58500 2.6 0.09999999999985551\n",
      "59000 2.0 0.09999999999985551\n",
      "59500 1.9 0.09999999999985551\n",
      "60000 1.8 0.09999999999985551\n",
      "60500 2.2 0.09999999999985551\n",
      "61000 1.4 0.09999999999985551\n",
      "61500 3.0 0.09999999999985551\n",
      "62000 3.5 0.09999999999985551\n",
      "62500 1.7 0.09999999999985551\n",
      "63000 2.7 0.09999999999985551\n",
      "63500 1.5 0.09999999999985551\n",
      "64000 1.7 0.09999999999985551\n",
      "64500 1.9 0.09999999999985551\n",
      "65000 2.0 0.09999999999985551\n",
      "65500 3.5 0.09999999999985551\n",
      "66000 2.1 0.09999999999985551\n",
      "66500 2.4 0.09999999999985551\n",
      "67000 2.6 0.09999999999985551\n",
      "67500 2.9 0.09999999999985551\n",
      "68000 2.4 0.09999999999985551\n",
      "68500 1.4 0.09999999999985551\n",
      "69000 1.7 0.09999999999985551\n",
      "69500 2.2 0.09999999999985551\n",
      "70000 3.4 0.09999999999985551\n",
      "70500 3.1 0.09999999999985551\n",
      "71000 2.6 0.09999999999985551\n",
      "71500 2.2 0.09999999999985551\n",
      "72000 2.2 0.09999999999985551\n",
      "72500 3.5 0.09999999999985551\n",
      "73000 3.9 0.09999999999985551\n",
      "73500 3.1 0.09999999999985551\n",
      "74000 2.6 0.09999999999985551\n",
      "74500 2.7 0.09999999999985551\n",
      "75000 2.7 0.09999999999985551\n",
      "75500 2.6 0.09999999999985551\n",
      "76000 3.2 0.09999999999985551\n",
      "76500 3.8 0.09999999999985551\n",
      "77000 2.8 0.09999999999985551\n",
      "77500 3.3 0.09999999999985551\n",
      "78000 2.2 0.09999999999985551\n",
      "78500 2.0 0.09999999999985551\n",
      "79000 4.6 0.09999999999985551\n",
      "79500 2.9 0.09999999999985551\n",
      "80000 1.3 0.09999999999985551\n",
      "80500 4.2 0.09999999999985551\n",
      "81000 6.7 0.09999999999985551\n",
      "81500 4.0 0.09999999999985551\n",
      "82000 3.7 0.09999999999985551\n",
      "82500 4.0 0.09999999999985551\n",
      "83000 4.9 0.09999999999985551\n",
      "83500 5.0 0.09999999999985551\n",
      "84000 7.5 0.09999999999985551\n",
      "84500 4.4 0.09999999999985551\n",
      "85000 4.2 0.09999999999985551\n",
      "85500 7.3 0.09999999999985551\n",
      "86000 6.3 0.09999999999985551\n",
      "86500 5.6 0.09999999999985551\n",
      "87000 5.8 0.09999999999985551\n",
      "87500 4.9 0.09999999999985551\n",
      "88000 6.5 0.09999999999985551\n",
      "88500 4.1 0.09999999999985551\n",
      "89000 5.7 0.09999999999985551\n",
      "89500 3.0 0.09999999999985551\n",
      "90000 7.9 0.09999999999985551\n",
      "90500 6.2 0.09999999999985551\n",
      "91000 6.5 0.09999999999985551\n",
      "91500 5.5 0.09999999999985551\n",
      "92000 7.3 0.09999999999985551\n",
      "92500 6.9 0.09999999999985551\n",
      "93000 7.8 0.09999999999985551\n",
      "93500 6.9 0.09999999999985551\n",
      "94000 6.2 0.09999999999985551\n",
      "94500 8.3 0.09999999999985551\n",
      "95000 8.0 0.09999999999985551\n",
      "95500 5.7 0.09999999999985551\n",
      "96000 7.3 0.09999999999985551\n",
      "96500 6.5 0.09999999999985551\n",
      "97000 8.3 0.09999999999985551\n",
      "97500 8.3 0.09999999999985551\n",
      "98000 6.6 0.09999999999985551\n",
      "98500 9.8 0.09999999999985551\n",
      "99000 8.5 0.09999999999985551\n",
      "99500 8.3 0.09999999999985551\n",
      "100000 7.5 0.09999999999985551\n",
      "Saved Model\n",
      "100500 7.1 0.09999999999985551\n",
      "101000 10.1 0.09999999999985551\n",
      "101500 9.2 0.09999999999985551\n",
      "102000 8.0 0.09999999999985551\n",
      "102500 7.5 0.09999999999985551\n",
      "103000 9.5 0.09999999999985551\n",
      "103500 9.4 0.09999999999985551\n",
      "104000 7.8 0.09999999999985551\n",
      "104500 8.3 0.09999999999985551\n",
      "105000 11.5 0.09999999999985551\n",
      "105500 11.2 0.09999999999985551\n",
      "106000 10.1 0.09999999999985551\n",
      "106500 8.5 0.09999999999985551\n",
      "107000 14.6 0.09999999999985551\n",
      "107500 10.4 0.09999999999985551\n",
      "108000 10.6 0.09999999999985551\n",
      "108500 13.0 0.09999999999985551\n",
      "109000 12.2 0.09999999999985551\n",
      "109500 11.9 0.09999999999985551\n",
      "110000 12.6 0.09999999999985551\n",
      "110500 13.7 0.09999999999985551\n",
      "111000 13.3 0.09999999999985551\n",
      "111500 11.0 0.09999999999985551\n",
      "112000 16.2 0.09999999999985551\n",
      "112500 11.5 0.09999999999985551\n",
      "113000 12.5 0.09999999999985551\n",
      "113500 15.3 0.09999999999985551\n",
      "114000 14.9 0.09999999999985551\n",
      "114500 13.9 0.09999999999985551\n",
      "115000 13.5 0.09999999999985551\n",
      "115500 14.6 0.09999999999985551\n",
      "116000 15.5 0.09999999999985551\n",
      "116500 15.2 0.09999999999985551\n",
      "117000 13.6 0.09999999999985551\n",
      "117500 17.4 0.09999999999985551\n",
      "118000 16.3 0.09999999999985551\n",
      "118500 18.4 0.09999999999985551\n",
      "119000 15.7 0.09999999999985551\n",
      "119500 17.7 0.09999999999985551\n",
      "120000 14.4 0.09999999999985551\n",
      "120500 17.6 0.09999999999985551\n",
      "121000 15.7 0.09999999999985551\n",
      "121500 18.4 0.09999999999985551\n",
      "122000 17.2 0.09999999999985551\n",
      "122500 16.9 0.09999999999985551\n",
      "123000 16.5 0.09999999999985551\n",
      "123500 17.2 0.09999999999985551\n",
      "124000 14.0 0.09999999999985551\n",
      "124500 15.5 0.09999999999985551\n",
      "125000 18.4 0.09999999999985551\n",
      "125500 17.0 0.09999999999985551\n",
      "126000 19.8 0.09999999999985551\n",
      "126500 15.7 0.09999999999985551\n",
      "127000 19.9 0.09999999999985551\n",
      "127500 21.7 0.09999999999985551\n",
      "128000 18.6 0.09999999999985551\n",
      "128500 18.8 0.09999999999985551\n",
      "129000 14.7 0.09999999999985551\n",
      "129500 20.3 0.09999999999985551\n",
      "130000 19.0 0.09999999999985551\n",
      "130500 17.9 0.09999999999985551\n",
      "131000 19.0 0.09999999999985551\n",
      "131500 19.2 0.09999999999985551\n",
      "132000 18.4 0.09999999999985551\n",
      "132500 18.8 0.09999999999985551\n",
      "133000 17.4 0.09999999999985551\n",
      "133500 16.7 0.09999999999985551\n",
      "134000 19.1 0.09999999999985551\n",
      "134500 18.0 0.09999999999985551\n",
      "135000 20.5 0.09999999999985551\n",
      "135500 21.7 0.09999999999985551\n",
      "136000 21.7 0.09999999999985551\n",
      "136500 15.8 0.09999999999985551\n",
      "137000 17.9 0.09999999999985551\n",
      "137500 18.0 0.09999999999985551\n",
      "138000 19.5 0.09999999999985551\n",
      "138500 19.9 0.09999999999985551\n",
      "139000 14.1 0.09999999999985551\n",
      "139500 17.7 0.09999999999985551\n",
      "140000 18.3 0.09999999999985551\n",
      "140500 20.2 0.09999999999985551\n",
      "141000 20.0 0.09999999999985551\n",
      "141500 19.7 0.09999999999985551\n",
      "142000 20.5 0.09999999999985551\n",
      "142500 19.5 0.09999999999985551\n",
      "143000 20.1 0.09999999999985551\n",
      "143500 20.0 0.09999999999985551\n",
      "144000 20.2 0.09999999999985551\n",
      "144500 19.8 0.09999999999985551\n",
      "145000 19.2 0.09999999999985551\n",
      "145500 20.3 0.09999999999985551\n",
      "146000 20.5 0.09999999999985551\n",
      "146500 18.0 0.09999999999985551\n",
      "147000 15.7 0.09999999999985551\n",
      "147500 20.2 0.09999999999985551\n",
      "148000 20.4 0.09999999999985551\n",
      "148500 20.6 0.09999999999985551\n",
      "149000 22.5 0.09999999999985551\n",
      "149500 21.3 0.09999999999985551\n",
      "150000 21.4 0.09999999999985551\n",
      "Saved Model\n",
      "150500 22.3 0.09999999999985551\n",
      "151000 23.2 0.09999999999985551\n",
      "151500 19.1 0.09999999999985551\n",
      "152000 20.7 0.09999999999985551\n",
      "152500 22.8 0.09999999999985551\n",
      "153000 21.2 0.09999999999985551\n",
      "153500 19.8 0.09999999999985551\n",
      "154000 20.3 0.09999999999985551\n",
      "154500 21.5 0.09999999999985551\n",
      "155000 20.5 0.09999999999985551\n",
      "155500 20.7 0.09999999999985551\n",
      "156000 21.3 0.09999999999985551\n",
      "156500 21.1 0.09999999999985551\n",
      "157000 17.5 0.09999999999985551\n",
      "157500 21.2 0.09999999999985551\n",
      "158000 21.2 0.09999999999985551\n",
      "158500 21.0 0.09999999999985551\n",
      "159000 18.1 0.09999999999985551\n",
      "159500 21.4 0.09999999999985551\n",
      "160000 22.8 0.09999999999985551\n",
      "160500 22.4 0.09999999999985551\n",
      "161000 21.1 0.09999999999985551\n",
      "161500 22.3 0.09999999999985551\n",
      "162000 20.3 0.09999999999985551\n",
      "162500 20.4 0.09999999999985551\n",
      "163000 21.2 0.09999999999985551\n",
      "163500 20.8 0.09999999999985551\n",
      "164000 22.1 0.09999999999985551\n",
      "164500 19.5 0.09999999999985551\n",
      "165000 22.9 0.09999999999985551\n",
      "165500 22.3 0.09999999999985551\n",
      "166000 22.1 0.09999999999985551\n",
      "166500 21.3 0.09999999999985551\n",
      "167000 20.7 0.09999999999985551\n",
      "167500 22.2 0.09999999999985551\n",
      "168000 22.8 0.09999999999985551\n",
      "168500 18.8 0.09999999999985551\n",
      "169000 22.5 0.09999999999985551\n",
      "169500 22.1 0.09999999999985551\n",
      "170000 24.1 0.09999999999985551\n",
      "170500 22.0 0.09999999999985551\n",
      "171000 21.5 0.09999999999985551\n",
      "171500 22.8 0.09999999999985551\n",
      "172000 21.5 0.09999999999985551\n",
      "172500 22.4 0.09999999999985551\n",
      "173000 21.7 0.09999999999985551\n"
     ]
    }
   ],
   "source": [
    "# 그래프를 초기화한다\n",
    "tf.reset_default_graph()\n",
    "# 주요 신경망을 만든다\n",
    "mainQN = Qnetwork(h_size)\n",
    "# 타겟 신경망을 만든다.\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "# 변수들을 초기화한다\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# saver를 만든다\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# 학습가능한 변수를 꺼낸다\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "# 타겟 신경망을 업데이트하기 위한 값을 만든다\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "# 경험을 저장할 장소\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# 무작위 행위 확률을 설정한다\n",
    "e = startE\n",
    "# 점점 줄여나간다.\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "# 에피소드별 총 보상과 걸음을 저장할 리스트를 만든다\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "# 모델을 세이브할 장소를 만든다.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# 텐서플로 세션을 연다\n",
    "with tf.Session() as sess:\n",
    "    # 모델을 불러올지 체크\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        # 모델을 불러온다\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    # 변수를 초기화한다.\n",
    "    sess.run(init)\n",
    "    # 주요 신경망과 동일하게 타겟 신경망을 설정한다\n",
    "    updateTarget(targetOps,sess) \n",
    "    # 에피소드 시작\n",
    "    for i in range(num_episodes):\n",
    "        # 에피소드별 경험 버퍼를 초기화한다\n",
    "        episodeBuffer = experience_buffer()\n",
    "        \n",
    "        # 환경과 처음 상태을 초기화한다\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        # 종료 여부\n",
    "        d = False\n",
    "        # 보상\n",
    "        rAll = 0\n",
    "        # 걸음\n",
    "        j = 0\n",
    "        # Q-Network\n",
    "        # 만약 50 걸음보다 더 간다면 종료한다.\n",
    "        while j < max_epLength:\n",
    "            j+=1\n",
    "            # Q-network 로부터 행동을 greedy 하게 선택하거나 e의 확률로 무작위 행동을 한다\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                # 신경망을 통해 Q 값을 가져오는 부분\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            \n",
    "            # 주어진 행동을 실행하고 다음 상태, 보상, 종료 여부를 가져옴\n",
    "            s1,r,d = env.step(a)\n",
    "            # 상태를 다시 21168 차원으로 리사이즈\n",
    "            s1 = processState(s1)\n",
    "            # 걸음수를 늘림\n",
    "            total_steps += 1\n",
    "            # 버퍼에 현재 상태, 행동, 보상, 다음 상태, 종료 여부를 저장한다\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) \n",
    "            \n",
    "            # 무작위 행동의 수를 넘으면 시작\n",
    "            if total_steps > pre_train_steps:\n",
    "                # 무작위 확률 값을 줄인다\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                # 총 걸음이 업데이트 수로 나누어 떨어지면 시작\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    # 경험으로부터 랜덤한 배치를 뽑는다\n",
    "                    trainBatch = myBuffer.sample(batch_size) \n",
    "                    # 아래는 target Q-value를 업데이트하는 Double-DQN을 수행한다\n",
    "                    # 주요 신경망에서 행동을 고른다.\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    # 타겟 신경망에서 Q 값들을 얻는다.\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    # 종료 여부에 따라 가짜 라벨을 만들어준다\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    # 타겟 신경망의 Q 값들 중에 주요 신경망에서 고른 행동 번째의 Q 값들을 가져온다.(이부분이 doubleQ)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    # 보상에 대한 더블 Q 값을 더해준다. y는 할인 인자\n",
    "                    # targetQ 는 즉각적인 보상 + 다음 상태의 최대 보상(doubleQ)\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    # 우리의 타겟 값들과 함께 신경망을 업데이트해준다.\n",
    "                    # 행동들에 대해서 targetQ 값과의 차이를 통해 손실을 구하고 업데이트\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    \n",
    "                    # 주요 신경망과 동일하게 타겟 신경망을 설정한다.\n",
    "                    # 물론 주요 신경망의 값은 tau 만큼만이 반영된다.\n",
    "                    updateTarget(targetOps,sess) \n",
    "            # 총 봊상\n",
    "            rAll += r\n",
    "            # 상태를 바꾼다.\n",
    "            s = s1\n",
    "            \n",
    "            # 종료가 되면 멈춘다.\n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        # 이 에피소드로부터의 모든 경험을 저장한다\n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        # 걸음을 저장한다.\n",
    "        jList.append(j)\n",
    "        # 보상을 저장한다\n",
    "        rList.append(rAll)\n",
    "        # 주기적으로 모델을 저장한다\n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model\")\n",
    "        # 최근 10 에피소드의 평균 보상값을 나타낸다.\n",
    "        if len(rList) % 10 == 0:\n",
    "            print (total_steps,np.mean(rList[-10:]), e)\n",
    "    # 모델을 저장한다.\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "# 성공확률을 표시\n",
    "print (\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
