{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning in Tensorflow Part 2: Policy Gradient Method\n",
    "\n",
    "이 튜토리얼은 CartPole 문제를 해결할 수 있는 정책-그라디언트 기반의 에이전트를 만드는 방법에 대한 예제를 담고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle # python 3.5\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole 환경 불러오기\n",
    "\n",
    "만약 OpenAI gym 을 아직 설치하지 않았다면, pip install gym 으로 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-11 16:11:40,192] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "무작위 행동으로 환경을 동작시킨다면 잘 될까?\n",
    "(hint: 잘 안됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for this episode was: 14.0\n",
      "Reward for this episode was: 15.0\n",
      "Reward for this episode was: 29.0\n",
      "Reward for this episode was: 23.0\n",
      "Reward for this episode was: 28.0\n",
      "Reward for this episode was: 14.0\n",
      "Reward for this episode was: 71.0\n",
      "Reward for this episode was: 11.0\n",
      "Reward for this episode was: 30.0\n",
      "Reward for this episode was: 42.0\n"
     ]
    }
   ],
   "source": [
    "# 환경을 초기화한다. 초기 상태(state, observation을 만드는 env.reset())\n",
    "env.reset()\n",
    "# 몇번 에피소드 할것인지 기록\n",
    "random_episodes = 0\n",
    "# 보상합 기록\n",
    "reward_sum = 0\n",
    "# 10번 동안 랜덤하게 함\n",
    "while random_episodes < 10:\n",
    "    # env.render() 는 에이전트가 하는 것을 볼 수 있게 화면에 나타내준다.\n",
    "    env.render()\n",
    "    # env.step()은 주어진 행동을 받아 다음 상태와 보상을 반환한다.\n",
    "    observation, reward, done, _ = env.step(np.random.randint(0,2))\n",
    "    # 보상합을 기록한다.\n",
    "    reward_sum += reward\n",
    "    # 에피소드가 끝나면(막대가 쓰러지면 done 이 True가 됨)\n",
    "    if done:\n",
    "        # 에피소드 보상 나타내고 초기화\n",
    "        random_episodes += 1\n",
    "        print (\"Reward for this episode was:\",reward_sum)\n",
    "        reward_sum = 0\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 작업의 목표는 에피소드별 200의 보상을 성취하는 것이다. 에이전트는 막대를 공중에서 유지하는 매 스탭마다 +1의 보상을 받는다. 랜덤한 행동을 선택함으로써 우리는 에피소드별 보상은 24 정도다. 강화학습과 함께 더 좋게 만들어보자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Neural Network agent\n",
    "\n",
    "이번엔 상태를 갖고, 은닉층 하나에 보내서 오른쪽 왼쪽 행동을 선택할 확률을 제시하는 정책 신경망을 이용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "H = 10 # 은닉층의 노드 수\n",
    "batch_size = 5 # 몇개의 에피소드마다 파라미터를 업데이트할 것인지\n",
    "learning_rate = 1e-2 # 학습률\n",
    "gamma = 0.99 # 보상에 대한 할인 인자\n",
    "\n",
    "D = 4 # 입력 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 그래프를 초기화한다\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 관찰은 상태를 받는다.\n",
    "observations = tf.placeholder(tf.float32, [None,D] , name=\"input_x\")\n",
    "# W1은 은닉층으로 보낸다\n",
    "W1 = tf.get_variable(\"W1\", shape=[D, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "# relu 활성화함수를 쓴다\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "# 은닉층의 결과인 10개의 값으로 하나의 결과값(점수)을 낸다\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "# 점수를 확률로 변환한다.\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "# 학습 가능한 변수들 (가중치)\n",
    "tvars = tf.trainable_variables()\n",
    "# 출력값을 받는 부분\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "# 이득을 받는 부분\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "# 손실함수. 좋은 이득(시간 경과에 따른 보상)을 더 자주 주는 행동으로 \n",
    "# 가중치를 보내고, 덜 가능성이 있는 행동에 가중치를 보낸다.\n",
    "\n",
    "# 왜 이게 동작하는 것일까?\n",
    "# cross entropy와 비슷하다. 내가 행동을 1로 했고, 그 행동에 높은 확률을 주었다면 손실이 작고,\n",
    "# 내가 0으로 움직였고, 그 행동에 낮은 확률을 주었다면 손실이 작다.\n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "# 위의 각 행동의 잘하고 못하고 부분을 지연된 보상으로 조정하고 난 모든 것을 손실로 본다.\n",
    "loss = -tf.reduce_mean(loglik * advantages) \n",
    "# 이 손실을 이용해 학습 변수들의 그라디언트를 구한다.\n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "# Once we have collected a series of gradients from multiple episodes, we apply them.\n",
    "# We don't just apply gradeients after every episode in order to account for noise in the reward signal.\n",
    "# 여러 에피소드로부터의 그라디언트를 모았다가 그것을 적용한다.\n",
    "# 왜 매 에피소드마다 그라디언트를 업데이트하지 않느냐면 에피소드의 노이즈까지 학습할까봐\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate) # 최적화기 adam\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\") # 그라디언트 저장하는 부분\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "# 그라디언트 적용하는 부분\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage function\n",
    "\n",
    "이 함수는 에이전트가 받은 보상에 가중치를 허용한다. Cart-pole 작업에서 우리가 원하는 행동은 공중에서 가장 오래 막대를 유지하는 것에 큰 보상을 갖고, 떨어지는 데 기여한 행동에 감소하거나 부정적 보상을 갖기를 원한다. 우리는 이것을 에피소드 끝에서부터 보상에 가중치를 주는 방식으로 할 수 있다. 에피소드가 끝난 후에는 그 행동이 부정적이었는지 알 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 할인 함수\n",
    "def discount_rewards(r):\n",
    "    \"\"\" 보상 배열을 받아 할인된 보상을 계산한다\"\"\"\n",
    "    # 할인된 보상을 전부 0으로 초기화\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    # 보상을 역순으로 더해가며, 가중치를 준다.\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 20.000000.  Total average reward 20.000000.\n",
      "Average reward for episode 17.400000.  Total average reward 19.974000.\n",
      "Average reward for episode 14.600000.  Total average reward 19.920260.\n",
      "Average reward for episode 20.600000.  Total average reward 19.927057.\n",
      "Average reward for episode 23.600000.  Total average reward 19.963787.\n",
      "Average reward for episode 26.600000.  Total average reward 20.030149.\n",
      "Average reward for episode 45.600000.  Total average reward 20.285847.\n",
      "Average reward for episode 24.200000.  Total average reward 20.324989.\n",
      "Average reward for episode 26.600000.  Total average reward 20.387739.\n",
      "Average reward for episode 27.200000.  Total average reward 20.455862.\n",
      "Average reward for episode 34.800000.  Total average reward 20.599303.\n",
      "Average reward for episode 23.800000.  Total average reward 20.631310.\n",
      "Average reward for episode 13.400000.  Total average reward 20.558997.\n",
      "Average reward for episode 28.800000.  Total average reward 20.641407.\n",
      "Average reward for episode 20.600000.  Total average reward 20.640993.\n",
      "Average reward for episode 38.800000.  Total average reward 20.822583.\n",
      "Average reward for episode 25.200000.  Total average reward 20.866357.\n",
      "Average reward for episode 16.000000.  Total average reward 20.817694.\n",
      "Average reward for episode 27.600000.  Total average reward 20.885517.\n",
      "Average reward for episode 38.000000.  Total average reward 21.056661.\n",
      "Average reward for episode 19.400000.  Total average reward 21.040095.\n",
      "Average reward for episode 27.800000.  Total average reward 21.107694.\n",
      "Average reward for episode 38.400000.  Total average reward 21.280617.\n",
      "Average reward for episode 32.600000.  Total average reward 21.393811.\n",
      "Average reward for episode 35.400000.  Total average reward 21.533873.\n",
      "Average reward for episode 35.200000.  Total average reward 21.670534.\n",
      "Average reward for episode 23.800000.  Total average reward 21.691829.\n",
      "Average reward for episode 34.800000.  Total average reward 21.822910.\n",
      "Average reward for episode 26.800000.  Total average reward 21.872681.\n",
      "Average reward for episode 25.800000.  Total average reward 21.911954.\n",
      "Average reward for episode 19.400000.  Total average reward 21.886835.\n",
      "Average reward for episode 26.000000.  Total average reward 21.927967.\n",
      "Average reward for episode 27.000000.  Total average reward 21.978687.\n",
      "Average reward for episode 42.600000.  Total average reward 22.184900.\n",
      "Average reward for episode 31.000000.  Total average reward 22.273051.\n",
      "Average reward for episode 25.000000.  Total average reward 22.300321.\n",
      "Average reward for episode 47.200000.  Total average reward 22.549317.\n",
      "Average reward for episode 18.600000.  Total average reward 22.509824.\n",
      "Average reward for episode 30.200000.  Total average reward 22.586726.\n",
      "Average reward for episode 20.800000.  Total average reward 22.568859.\n",
      "Average reward for episode 32.400000.  Total average reward 22.667170.\n",
      "Average reward for episode 37.000000.  Total average reward 22.810498.\n",
      "Average reward for episode 27.600000.  Total average reward 22.858393.\n",
      "Average reward for episode 22.000000.  Total average reward 22.849809.\n",
      "Average reward for episode 27.400000.  Total average reward 22.895311.\n",
      "Average reward for episode 28.400000.  Total average reward 22.950358.\n",
      "Average reward for episode 26.800000.  Total average reward 22.988855.\n",
      "Average reward for episode 26.400000.  Total average reward 23.022966.\n",
      "Average reward for episode 35.000000.  Total average reward 23.142736.\n",
      "Average reward for episode 36.200000.  Total average reward 23.273309.\n",
      "Average reward for episode 32.000000.  Total average reward 23.360576.\n",
      "Average reward for episode 26.200000.  Total average reward 23.388970.\n",
      "Average reward for episode 29.000000.  Total average reward 23.445081.\n",
      "Average reward for episode 22.800000.  Total average reward 23.438630.\n",
      "Average reward for episode 27.800000.  Total average reward 23.482243.\n",
      "Average reward for episode 27.200000.  Total average reward 23.519421.\n",
      "Average reward for episode 28.000000.  Total average reward 23.564227.\n",
      "Average reward for episode 35.200000.  Total average reward 23.680584.\n",
      "Average reward for episode 47.600000.  Total average reward 23.919779.\n",
      "Average reward for episode 32.000000.  Total average reward 24.000581.\n",
      "Average reward for episode 28.800000.  Total average reward 24.048575.\n",
      "Average reward for episode 19.200000.  Total average reward 24.000089.\n",
      "Average reward for episode 36.400000.  Total average reward 24.124088.\n",
      "Average reward for episode 36.600000.  Total average reward 24.248848.\n",
      "Average reward for episode 38.800000.  Total average reward 24.394359.\n",
      "Average reward for episode 34.800000.  Total average reward 24.498415.\n",
      "Average reward for episode 34.800000.  Total average reward 24.601431.\n",
      "Average reward for episode 51.000000.  Total average reward 24.865417.\n",
      "Average reward for episode 51.400000.  Total average reward 25.130763.\n",
      "Average reward for episode 27.200000.  Total average reward 25.151455.\n",
      "Average reward for episode 32.800000.  Total average reward 25.227941.\n",
      "Average reward for episode 52.000000.  Total average reward 25.495661.\n",
      "Average reward for episode 52.400000.  Total average reward 25.764705.\n",
      "Average reward for episode 70.800000.  Total average reward 26.215058.\n",
      "Average reward for episode 30.200000.  Total average reward 26.254907.\n",
      "Average reward for episode 37.000000.  Total average reward 26.362358.\n",
      "Average reward for episode 45.200000.  Total average reward 26.550734.\n",
      "Average reward for episode 40.400000.  Total average reward 26.689227.\n",
      "Average reward for episode 39.400000.  Total average reward 26.816335.\n",
      "Average reward for episode 31.200000.  Total average reward 26.860171.\n",
      "Average reward for episode 42.400000.  Total average reward 27.015570.\n",
      "Average reward for episode 38.800000.  Total average reward 27.133414.\n",
      "Average reward for episode 45.200000.  Total average reward 27.314080.\n",
      "Average reward for episode 76.800000.  Total average reward 27.808939.\n",
      "Average reward for episode 48.000000.  Total average reward 28.010850.\n",
      "Average reward for episode 55.400000.  Total average reward 28.284741.\n",
      "Average reward for episode 50.800000.  Total average reward 28.509894.\n",
      "Average reward for episode 48.800000.  Total average reward 28.712795.\n",
      "Average reward for episode 40.400000.  Total average reward 28.829667.\n",
      "Average reward for episode 40.000000.  Total average reward 28.941370.\n",
      "Average reward for episode 80.600000.  Total average reward 29.457956.\n",
      "Average reward for episode 52.400000.  Total average reward 29.687377.\n",
      "Average reward for episode 60.600000.  Total average reward 29.996503.\n",
      "Average reward for episode 41.400000.  Total average reward 30.110538.\n",
      "Average reward for episode 63.000000.  Total average reward 30.439433.\n",
      "Average reward for episode 83.400000.  Total average reward 30.969038.\n",
      "Average reward for episode 69.600000.  Total average reward 31.355348.\n",
      "Average reward for episode 48.200000.  Total average reward 31.523795.\n",
      "Average reward for episode 40.200000.  Total average reward 31.610557.\n",
      "Average reward for episode 60.000000.  Total average reward 31.894451.\n",
      "Average reward for episode 39.200000.  Total average reward 31.967507.\n",
      "Average reward for episode 53.400000.  Total average reward 32.181831.\n",
      "Average reward for episode 54.800000.  Total average reward 32.408013.\n",
      "Average reward for episode 40.400000.  Total average reward 32.487933.\n",
      "Average reward for episode 35.200000.  Total average reward 32.515054.\n",
      "Average reward for episode 40.000000.  Total average reward 32.589903.\n",
      "Average reward for episode 75.600000.  Total average reward 33.020004.\n",
      "Average reward for episode 65.600000.  Total average reward 33.345804.\n",
      "Average reward for episode 64.800000.  Total average reward 33.660346.\n",
      "Average reward for episode 51.400000.  Total average reward 33.837743.\n",
      "Average reward for episode 42.400000.  Total average reward 33.923365.\n",
      "Average reward for episode 72.800000.  Total average reward 34.312131.\n",
      "Average reward for episode 66.600000.  Total average reward 34.635010.\n",
      "Average reward for episode 85.200000.  Total average reward 35.140660.\n",
      "Average reward for episode 78.400000.  Total average reward 35.573253.\n",
      "Average reward for episode 90.800000.  Total average reward 36.125521.\n",
      "Average reward for episode 83.800000.  Total average reward 36.602266.\n",
      "Average reward for episode 71.000000.  Total average reward 36.946243.\n",
      "Average reward for episode 65.400000.  Total average reward 37.230781.\n",
      "Average reward for episode 65.400000.  Total average reward 37.512473.\n",
      "Average reward for episode 65.200000.  Total average reward 37.789348.\n",
      "Average reward for episode 72.000000.  Total average reward 38.131455.\n",
      "Average reward for episode 83.600000.  Total average reward 38.586140.\n",
      "Average reward for episode 92.600000.  Total average reward 39.126279.\n",
      "Average reward for episode 71.000000.  Total average reward 39.445016.\n",
      "Average reward for episode 94.400000.  Total average reward 39.994566.\n",
      "Average reward for episode 114.600000.  Total average reward 40.740620.\n",
      "Average reward for episode 122.200000.  Total average reward 41.555214.\n",
      "Average reward for episode 129.400000.  Total average reward 42.433662.\n",
      "Average reward for episode 51.200000.  Total average reward 42.521325.\n",
      "Average reward for episode 131.400000.  Total average reward 43.410112.\n",
      "Average reward for episode 124.600000.  Total average reward 44.222011.\n",
      "Average reward for episode 113.400000.  Total average reward 44.913791.\n",
      "Average reward for episode 140.800000.  Total average reward 45.872653.\n",
      "Average reward for episode 128.000000.  Total average reward 46.693926.\n",
      "Average reward for episode 89.600000.  Total average reward 47.122987.\n",
      "Average reward for episode 108.800000.  Total average reward 47.739757.\n",
      "Average reward for episode 74.800000.  Total average reward 48.010360.\n",
      "Average reward for episode 142.000000.  Total average reward 48.950256.\n",
      "Average reward for episode 138.800000.  Total average reward 49.848753.\n",
      "Average reward for episode 137.000000.  Total average reward 50.720266.\n",
      "Average reward for episode 127.000000.  Total average reward 51.483063.\n",
      "Average reward for episode 138.800000.  Total average reward 52.356233.\n",
      "Average reward for episode 160.000000.  Total average reward 53.432670.\n",
      "Average reward for episode 120.600000.  Total average reward 54.104344.\n",
      "Average reward for episode 229.400000.  Total average reward 55.857300.\n",
      "Task solved in 730 episodes!\n",
      "730 Episodes completed.\n"
     ]
    }
   ],
   "source": [
    "# 입력값들, 출력값들, 보상들을 저장하는 리스트\n",
    "xs,drs,ys = [],[],[]\n",
    "# 보상은 없다고 생각\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "# 에피소드 수를 기록\n",
    "episode_number = 1\n",
    "# 에피소드 몇번 할지 기록\n",
    "total_episodes = 10000\n",
    "# 텐서플로 변수를 초기화함\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 텐서플로 실행\n",
    "with tf.Session() as sess:\n",
    "    # 에이전트 표시 안함\n",
    "    rendering = False\n",
    "    # 변수 초기화\n",
    "    sess.run(init)\n",
    "    # 상태 초기화\n",
    "    observation = env.reset()\n",
    "\n",
    "    # 그라디언트 담을 곳 초기화\n",
    "    # 정책 신경망을 업데이트 하기 전까지 그라디언트를 모은다.\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    # 전부 0으로 초기화\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    \n",
    "    # 에피소드 시작\n",
    "    while episode_number <= total_episodes:\n",
    "        \n",
    "        # Rendering the environment slows things down, \n",
    "        # 에이전트를 표시하는 것은 학습을 느리게 한다\n",
    "        # 그래서 에이전트가 잘 작동할 때까지 표시 안한다\n",
    "        \n",
    "        if reward_sum/batch_size > 100 or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        # 상태를 신경망이 다룰 수 있는 형태로 바꿈\n",
    "        x = np.reshape(observation,[1,D])\n",
    "        \n",
    "        # 정책 신경망을 돌려서 액션에 대한 확률 값을 얻은\n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        # 확률 값보다 무작위 값이 작다면 1로 움직이고\n",
    "        # 그렇지 않다면 0으로 움직임\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "        xs.append(x) # 상태를 저장한다\n",
    "        y = 1 if action == 0 else 0 # 가짜 라벨, 각 행동에 대한 라벨을 저장함\n",
    "        ys.append(y)\n",
    "\n",
    "        # 새로운 상태와 보상을 얻음\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        \n",
    "        # 보상을 기록함\n",
    "        # step 함수를 부르고나면 이전 행동에 대한 보상을 잃기 때문에 기록함\n",
    "        drs.append(reward) \n",
    "        \n",
    "        # 에피소드가 끝나면\n",
    "        if done: \n",
    "            episode_number += 1\n",
    "            # 에피소드별 각 상태, 라벨, 보상으로 업데이트를 준비함\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            xs,drs,ys = [],[],[] # 다음 에피소드를 위해 초기화\n",
    "\n",
    "            # 시간에 대해 보상들을 할인함\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            # 보상들을 평균이 0이고 분산이 1이 되도록 정규화\n",
    "            # 그라디언트의 분산을 조절하는데 도움을 준다.\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "            \n",
    "            # 이 에피소드의 그라디언트를 구함\n",
    "            tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "            \n",
    "            # 그라디언트를 그라디언트 버퍼에 저장함\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # 충분한 에피소드(배치 사이즈)만큼이 끝나면,\n",
    "            # 그라디언트 버퍼에 저장된 그라디언트를 신경망에 적용함\n",
    "            if episode_number % batch_size == 0: \n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                # 얼마나 우리 신경망이 에피소드 별로 잘하는지 통계를 냄\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                \n",
    "                print ('Average reward for episode %f.  Total average reward %f.' % (reward_sum/batch_size, running_reward/batch_size))\n",
    "                \n",
    "                # 에피소드별 평균 보상이 200을 넘으면 멈춤\n",
    "                if reward_sum/batch_size > 200: \n",
    "                    print (\"Task solved in\",episode_number,'episodes!')\n",
    "                    break\n",
    "                \n",
    "                # 보상을 초기화\n",
    "                reward_sum = 0\n",
    "            #상태를 초기화\n",
    "            observation = env.reset()\n",
    "        \n",
    "print (episode_number,'Episodes completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망이 무작위 행동보다 잘할 뿐만 아니라 각 에피소드마다 200 점을 성취하는 것을 볼 수 있다.\n",
    "그러므로 작업 성공!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
